{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/text-tagging-with-llama3/blob/main/llama3_tagging_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install package and load the extension\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm\n",
        "\n",
        "# #- Install/run Ollama in the terminal\n",
        "\n",
        "# !curl -fsSL https://ollama.com/install.sh | sh\n",
        "# !ollama serve & ollama pull llama3"
      ],
      "metadata": {
        "id": "C99IPN_LY-jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tItATjLXHnKu"
      },
      "outputs": [],
      "source": [
        "#@title Install libraries\n",
        "\n",
        "#LangChain related libraries\n",
        "!pip install -q -U langchain langchain_experimental\n",
        "\n",
        "!pip install -q -U gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title tagging config\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# tagging prompt template\n",
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "Extract the desired information from the  passage given in triple Backticks.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Please follow the response format as given.\n",
        "\n",
        "Passage:\n",
        "'''{input}'''\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Output Schema for structured response\n",
        "class Classification(BaseModel):\n",
        "    sentiment: str = Field(description=\"The sentiment of the text\")\n",
        "    aggressiveness: int = Field(\n",
        "        description=\"How aggressive the text is on a scale from 1 to 10\"\n",
        "    )\n",
        "    language: str = Field(description=\"The language the text is written in\")\n",
        "    confidence_score: float = Field(\n",
        "        default=None, description=\"Confidence in sentiment prediction (0-1)\"\n",
        "    )\n",
        "    topic: str = Field(default=None, description=\"The main topic of the text\")\n",
        "    keywords: list[str] = Field(default=None, description=\"List of keywords found\")\n"
      ],
      "metadata": {
        "id": "P3__zrBQJeFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Chain with llama3\n",
        "\n",
        "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
        "\n",
        "# Chain\n",
        "llm = OllamaFunctions(model=\"llama3\", format=\"json\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(Classification)\n",
        "chain = tagging_prompt | structured_llm"
      ],
      "metadata": {
        "id": "OxdAumJ8N1N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add Gradio UI\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def gradio_fn(msg):\n",
        "    response = chain.invoke({\"input\": msg}).dict()\n",
        "    return response\n",
        "\n",
        "gr.Interface(\n",
        "    fn=gradio_fn,\n",
        "    inputs=gr.Textbox(lines=4, label=\"Input Text\"),\n",
        "    outputs=gr.Textbox(lines=4, label=\"Output Data\"),\n",
        "    theme=\"soft\"\n",
        "    ).launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "mixEVHEh56PK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}